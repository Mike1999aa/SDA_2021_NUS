{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import *\n",
    "from pandas import read_csv\n",
    "from pandas import ExcelFile\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data):\n",
    "    \n",
    "    #get data\n",
    "    Y=data['UpDownSign']\n",
    "    X=data.drop(columns=['UpDownSign','AdjClose'])\n",
    "    \n",
    "    #Split the data to train and test\n",
    "    Xtrain,Xtest,ytrain,ytest = train_test_split(X,Y, random_state = 17)\n",
    "    \n",
    "    '''\n",
    "    #Below uses K-Fold Validation method to find the best parameters\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 5)]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 'sqrt','log2']\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(5, 100, num = 15)]\n",
    "    max_depth.append(None)\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,}\n",
    "    \n",
    "    rf = RandomForestClassifier()\n",
    "    regr = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 5, verbose=2, random_state=41)\n",
    "    regr.fit(Xtrain,ytrain)\n",
    "    \n",
    "    print(regr.best_params_)\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    regr_best=RandomForestClassifier(n_estimators=300,min_samples_split=2,min_samples_leaf=4,max_features='log2',max_depth=50,random_state=17,oob_score=True)\n",
    "    regr_best.fit(Xtrain,ytrain)\n",
    "    ypred=regr_best.predict(Xtest)\n",
    "    \n",
    "    #print accuracy rate\n",
    "    print('Accuracy Score:',accuracy_score(ytest, ypred, normalize=True)*100.0)\n",
    "    \n",
    "    #print OOB score (R^2 for out-of-bag sample set)\n",
    "    print('OOB score',regr_best.oob_score_)\n",
    "    '''\n",
    "    regr_best=RandomForestClassifier(n_estimators=700,min_samples_split=5,min_samples_leaf=2,max_features='log2',max_depth=40,random_state=17,oob_score=True)\n",
    "    regr_best.fit(Xtrain,ytrain)\n",
    "    ypred=regr_best.predict(Xtest)\n",
    "    \n",
    "    '''\n",
    "    #print accuracy rate\n",
    "    print('Accuracy Score:',accuracy_score(ytest, ypred, normalize=True)*100.0)\n",
    "    \n",
    "    #print OOB score (R^2 for out-of-bag sample set)\n",
    "    print('OOB score',regr_best.oob_score_)\n",
    "    \n",
    "    #print report metrics\n",
    "    report = classification_report(ytest, ypred)\n",
    "    print(report)\n",
    "    \n",
    "    #print feature significance\n",
    "    feature_significance=pd.DataFrame(regr_best.feature_importances_)\n",
    "    feature_significance.index=X.columns\n",
    "    feature_significance.rename(columns={0:\"significance score\"},inplace=True)\n",
    "    feature_significance=feature_significance.sort_values(by='significance score',ascending=False)\n",
    "    \n",
    "    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "    plt.rcdefaults()\n",
    "    fig, ax = plt.subplots()\n",
    "    y_pos = np.arange(feature_significance.index.size)\n",
    "    ax.barh(feature_significance.index, feature_significance['significance score'])\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(feature_significance.index)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xlabel('Signicance Score')\n",
    "    plt.show()\n",
    "    '''\n",
    "    '''\n",
    "    #Below is an example of decision tree\n",
    "    estimator = regr_best.estimators_[100]\n",
    "    \n",
    "    # Export as dot file\n",
    "    export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = X.columns,\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "    # Convert to png using system command (requires Graphviz)\n",
    "    call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "    # Display in jupyter notebook\n",
    "    Image(filename = 'tree.png')\n",
    "    '''\n",
    "    \n",
    "    return regr_best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
